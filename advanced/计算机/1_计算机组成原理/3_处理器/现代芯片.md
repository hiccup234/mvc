
## 指令 + 运算 = CPU

    指令的执行步骤可以分为：Fetch（取指令）、Decode（指令译码）、Execute（执行指令），一直循环这3个步骤就形成了指令周期
    
    1、ALU：组合逻辑电路
    2、存储数据：锁存器和D触发器
    3、PC寄存器：计数器电路
    4、解码和寻址：译码器电路
    
## 指令流水线

    目前arm架构或Intel i7的CPU的流水线深度为14级，即把一条指令拆分成14个不同的阶段
    
## 冒险和预测
    
    流水线设计需要解决三代冒险：结构冒险、数据冒险、控制冒险
    
    结构冒险：通过增加资源来解决资源冲突（如：访问内存数据和取指令的冲突），增加更多的线路，更多的寄存器，更多的晶体管，更多的高速缓存等
    
    数据冒险：同时执行的多条指令之间存在数据依赖，通过`流水线停顿`或`操作数前推`来解决
                             
    控制冒险：乱序执行
    
## 超线程技术

    通过增加CPU核心的PC寄存器、指令寄存器、条件码寄存器等来并行维护两条指令的状态，但是指令译码器和ALU等还是只有一份，
    所以在线程A流水线停顿时，CPU的译码器和ALU等就是空闲状态，可以去执行线程B。
    
    Python的numpy库利用了SIMD指令（单指令多数据流：Single Instruction Multiple Data）以及MMX（矩阵数学扩展），
    所以才在人工智能、机器学习、数学计算方面大放异彩。
    
### CPU指令集
    
   * RISC：一般都是固定长度，UC Berkeley的大卫·帕特森（David Patterson）教授70年代提出，2/8原则，CPU80%时间都在运行20%的指令
                   
   * CISC：x86指令集的长度不是固定的，但随着ARM的竞争，Intel推出了微指令架构，把CISC指令在译码器中翻译成RISC指令，并把翻译结果缓存在L0 cache
   
   
### 为什么深度学习使用GPU而不是CPU
    
    GPU是专门为图形处理而设计的，图形处理涉及大量的计算，但是计算程序不需要太多的分支预测和乱序执行，所以GPU的设计可以去掉这部分
    结构简化后，GPU可以做到更多的核心（48或更多）来提升并行计算的能力，从而更好的支持数值计算（机器学习）。
    
### FPGA、ASIC
    
    FPGA：现场可编程门阵列（Field-Programmable Gate Array）
    
    ASIC：专用集成电路（Application-Specific Integrated Circuit）
                                       
    深度学习：1、深度学习的模型训练：追求吞吐率，时间要求不敏感（少则几分钟，多则几个月）
            2、深度学习的模型推断：计算简单（矩阵乘法加法等），追求低响应时间、低功耗
    TPU即是专门为模型推断设计的，没有取指令功能，仅作为协处理器，由Google推出，TPU是ASIC的实现。
    
### 虚拟机

    1、宿主机+模拟器：非常类似JVM和Android的模拟器（在X86架构下运行ARM指令），也是解释执行客户机的计算机指令，性能比较差，但是可以跨硬件。
    
    2、Type-1和Type-2：Type-2型虚拟机更多用在PC中，Type-1一般用在数据中心中，它们都放弃了跨硬件的特性。
      Type-1型虚拟机是嵌入到操作系统内核的一部分，是系统级程序，直接将客户机的指令（同架构）运行在硬件上，如：KVM、XEN或微软的Hyper-V。
      Type-2型则有一个虚拟机监视器层，所有客户机的指令先发给监视器，监视器再转给宿主操作系统。
      
    3、Docker：严格来说Docker并不是完整的虚拟机，它只是做到了资源的隔离，但是去掉了客户机的操作系统，节省了资源的开销。
      容器编排工具有：Kubernetes、Dockers Swarm等。