
## 指令 + 运算 = CPU

    指令的执行步骤可以分为：Fetch（取指令）、Decode（指令译码）、Execute（执行指令），一直循环这3个步骤就形成了指令周期
    
    1、ALU：组合逻辑电路
    2、存储数据：锁存器和D触发器
    3、PC寄存器：计数器电路
    4、解码和寻址：译码器电路
    
## 指令流水线

    目前arm架构或Intel i7的CPU的流水线深度为14级，即把一条指令拆分成14个不同的阶段（Pipeline Stage）
    
## 冒险和预测
    
    流水线设计需要解决三大冒险：结构冒险、数据冒险、控制冒险
    
    结构冒险：通过增加资源来解决资源冲突（如：访问内存数据和取指令的冲突），增加更多的线路，更多的寄存器，更多的晶体管，更多的高速缓存等
    
    数据冒险：同时执行的多条指令之间存在数据依赖 = 先写后读 + 先读后写 + 写后再写（数据依赖、反依赖、输出依赖）
             通过`流水线停顿(气泡)`或`操作数前推(转发：即ALU运算结果不写回寄存器而是直接作为入参参与下次计算)`来解决
                             
             乱序执行（Out-of-Order Execution），乱序仅仅发生在流水线的指令译码和数据写回寄存器之间。
             也就是说对于单核的CPU，从外部来看指令的执行仍然是顺序的。
    
    控制冒险：缩短分支延迟、分支预测（今天下雨了，明天还会下雨吗？）、动态分支预测
    
## 多发射 & 超标量

    程序的 CPU 执行时间 = 指令数 × CPI × Clock Cycle Time
    超长指令字设计（Very Long Instruction Word，VLIW）：因特尔与惠普合作，安腾IA-64，在编译器软件层面解决重排序问题，取指时取出指令包，包中有多个可以并行执行的指令
    
## 超线程技术 & SIMD

    通过增加CPU核心的PC寄存器、指令寄存器、条件码寄存器等来并行维护两条指令的状态，但是指令译码器和ALU等还是只有一份，
    所以在线程A流水线停顿时，CPU的译码器和ALU等就是空闲状态，可以去执行线程B，因为线程A和B的指令没有数据依赖。
    适合IO密集型CPU计算占比不高的应用，如：数据库系统，Web服务器等。
    
    Python的numpy库利用了SIMD指令（单指令多数据流：Single Instruction Multiple Data）以及MMX（矩阵数学扩展），
    所以才在人工智能、机器学习、数学计算(向量、矩阵)方面大放异彩。（Python中不要在循环中计算，而是把计算向量化）
    
    SIMD在获取数据和执行指令的时候，都做到了并行。在从内存里面读取数据的时候，SIMD 是一次性读取多个数据。
    
    SISD：单指令单数据， 多核CPU中MIMD：多指令多数据
    
>>> import numpy as np
>>> import timeit
>>> a = list(range(1000))
>>> b = np.array(range(1000))
>>> timeit.timeit("[i + 1 for i in a]", setup="from __main__ import a", number=1000000)
32.82800309999993
>>> timeit.timeit("np.add(1, b)", setup="from __main__ import np, b", number=1000000)
0.9787889999997788
>>>

### 异常与中断
    中断向量
    异常的分类：中断(IO信号)、陷阱(系统调用)、故障(计算溢出、程序加载缺页)和中止
    
### CPU指令集
    
   * RISC：一般都是固定长度，UC Berkeley的大卫·帕特森（David Patterson）教授70年代提出，2/8原则，CPU80%时间都在运行20%的指令
                   
   * CISC：x86指令集的长度不是固定的，但随着ARM的竞争，Intel推出了微指令架构，把CISC指令在译码器中翻译成RISC指令，
           并把翻译结果缓存在L0 cache，后续的流水线和超标量都是按RISC来设计的
   
    IBM 的 PowerPC，SUN 的 SPARC 均采用RISC架构
    
    ARM(Advanced RISC Machines)，ARM授权其他厂商生产基于arm架构和指令集生产芯片，但arm本身并不是开源的
    
    RISC-V：CPU届的Linux，由大卫·帕特森教授从伯克利退休之后主导，开源的硬件，平头哥的“玄铁”就是基于RSIC-V
   
### 为什么深度学习使用GPU而不是CPU
    
    GPU是专门为图形处理而设计的，图形处理涉及大量的计算，但是计算程序不需要太多的分支预测和乱序执行，所以GPU的设计可以去掉这部分
    结构简化后，GPU可以做到更多的核心（48或更多）来提升并行计算的能力，从而更好的支持数值计算（机器学习）。
    
### FPGA、ASIC
    
    FPGA：现场可编程门阵列（Field-Programmable Gate Array），跟传统的编程语言不同，FPGA天然是并行的。
    
    ASIC：专用集成电路（Application-Specific Integrated Circuit）
                                       
    深度学习：1、深度学习的模型训练：追求吞吐率，时间要求不敏感（少则几分钟，多则几个月）
             2、深度学习的模型推断：计算简单（矩阵乘法加法等），追求低响应时间、低功耗
    TPU即是专门为模型推断设计的，没有取指令功能，仅作为协处理器，由Google推出，TPU是ASIC的实现。
    https://arxiv.org/ftp/arxiv/papers/1704/1704.04760.pdf
    
### 虚拟机

    1、宿主机+模拟器：非常类似JVM和Android的模拟器（在X86架构下运行ARM指令），也是解释执行客户机的计算机指令，性能比较差，但是可以跨硬件。
    
    2、Type-1和Type-2：Type-2型虚拟机更多用在PC中，Type-1一般用在数据中心中，它们都放弃了跨硬件的特性。
      Type-1型虚拟机是嵌入到操作系统内核的一部分，是系统级程序，直接将客户机的指令（同架构）运行在硬件上，如：KVM、XEN或微软的Hyper-V。
      Type-2型则有一个虚拟机监视器层，所有客户机的指令先发给监视器，监视器再转给宿主操作系统。
      
    3、Docker：严格来说Docker并不是完整的虚拟机，它只是做到了资源的隔离，但是去掉了客户机的操作系统，节省了资源的开销。
      容器编排工具有：Kubernetes、Dockers Swarm等。
      
    现代的云计算跟60/70年代的分时系统的设计思路一脉相承。